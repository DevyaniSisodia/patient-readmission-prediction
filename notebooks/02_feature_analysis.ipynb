{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3bbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Feature Analysis for Patient Readmission Prediction\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook performs comprehensive feature analysis including:\\n\",\n",
    "    \"- Feature importance analysis\\n\",\n",
    "    \"- Feature selection techniques\\n\",\n",
    "    \"- Statistical tests\\n\",\n",
    "    \"- Feature engineering evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import required libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning libraries\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\\n\",\n",
    "    \"from sklearn.feature_selection import (\\n\",\n",
    "    \"    SelectKBest, chi2, f_classif, mutual_info_classif,\\n\",\n",
    "    \"    RFE, SelectFromModel, VarianceThreshold\\n\",\n",
    "    \")\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, LabelEncoder\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.metrics import classification_report, roc_auc_score\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Statistical tests\\n\",\n",
    "    \"from scipy.stats import chi2_contingency, ttest_ind, mannwhitneyu\\n\",\n",
    "    \"from scipy import stats\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Database\\n\",\n",
    "    \"from sqlalchemy import create_engine\\n\",\n",
    "    \"\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set styling\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"pd.set_option('display.max_columns', None)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Libraries imported successfully!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Data Loading and Preprocessing\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load processed data\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    # Try to load from feature engineering output\\n\",\n",
    "    \"    df = pd.read_csv('../data/processed/engineered_features.csv')\\n\",\n",
    "    \"    print(\\\"Loaded engineered features from CSV\\\")\\n\",\n",
    "    \"except FileNotFoundError:\\n\",\n",
    "    \"    try:\\n\",\n",
    "    \"        # Fallback to cleaned data\\n\",\n",
    "    \"        df = pd.read_csv('../data/processed/cleaned_data.csv')\\n\",\n",
    "    \"        print(\\\"Loaded cleaned data from CSV\\\")\\n\",\n",
    "    \"    except FileNotFoundError:\\n\",\n",
    "    \"        # Fallback to database\\n\",\n",
    "    \"        engine = create_engine('sqlite:///../database/patient_readmission.db')\\n\",\n",
    "    \"        query = \\\"\\\"\\\"\\n\",\n",
    "    \"        SELECT \\n\",\n",
    "    \"            p.patient_id,\\n\",\n",
    "    \"            p.age,\\n\",\n",
    "    \"            p.gender,\\n\",\n",
    "    \"            p.race,\\n\",\n",
    "    \"            p.admission_type_id,\\n\",\n",
    "    \"            p.discharge_disposition_id,\\n\",\n",
    "    \"            p.admission_source_id,\\n\",\n",
    "    \"            p.time_in_hospital as length_of_stay,\\n\",\n",
    "    \"            p.num_lab_procedures,\\n\",\n",
    "    \"            p.num_procedures,\\n\",\n",
    "    \"            p.num_medications,\\n\",\n",
    "    \"            p.number_outpatient,\\n\",\n",
    "    \"            p.number_emergency,\\n\",\n",
    "    \"            p.number_inpatient,\\n\",\n",
    "    \"            p.number_diagnoses as num_diagnoses,\\n\",\n",
    "    \"            p.max_glu_serum,\\n\",\n",
    "    \"            p.A1Cresult,\\n\",\n",
    "    \"            p.change,\\n\",\n",
    "    \"            p.diabetesMed,\\n\",\n",
    "    \"            p.readmitted,\\n\",\n",
    "    \"            CASE WHEN p.readmitted IN ('<30', 'YES') THEN 1 ELSE 0 END as readmitted_30_days\\n\",\n",
    "    \"        FROM patients p\\n\",\n",
    "    \"        WHERE p.age != '[0-10)'\\n\",\n",
    "    \"        \\\"\\\"\\\"\\n\",\n",
    "    \"        df = pd.read_sql_query(query, engine)\\n\",\n",
    "    \"        print(\\\"Loaded raw data from database\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Dataset shape: {df.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Columns: {list(df.columns)}\\\")\\n\",\n",
    "    \"print(f\\\"Target variable exists: {'readmitted_30_days' in df.columns}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Basic preprocessing\\n\",\n",
    "    \"print(\\\"=== DATA PREPROCESSING ===\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create target if doesn't exist\\n\",\n",
    "    \"if 'readmitted_30_days' not in df.columns and 'readmitted' in df.columns:\\n\",\n",
    "    \"    df['readmitted_30_days'] = (df['readmitted'] == '<30').astype(int)\\n\",\n",
    "    \"    print(\\\"Created readmitted_30_days target variable\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove patient_id for analysis (keep for tracking if needed)\\n\",\n",
    "    \"analysis_df = df.drop(columns=['patient_id'], errors='ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Handle target variable\\n\",\n",
    "    \"if 'readmitted_30_days' in analysis_df.columns:\\n\",\n",
    "    \"    target = 'readmitted_30_days'\\n\",\n",
    "    \"    y = analysis_df[target]\\n\",\n",
    "    \"    X = analysis_df.drop(columns=[target, 'readmitted'], errors='ignore')\\n\",\n",
    "    \"    print(f\\\"Features shape: {X.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"Target distribution: {y.value_counts().to_dict()}\\\")\\n\",\n",
    "    \"    print(f\\\"Class balance: {y.value_counts(normalize=True).round(3).to_dict()}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Warning: Target variable not found. Creating dummy target for analysis.\\\")\\n\",\n",
    "    \"    y = np.random.choice([0, 1], size=len(analysis_df), p=[0.85, 0.15])\\n\",\n",
    "    \"    X = analysis_df\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nMissing values per column:\\\")\\n\",\n",
    "    \"missing_info = X.isnull().sum()\\n\",\n",
    "    \"print(missing_info[missing_info > 0].sort_values(ascending=False))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Univariate Feature Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Separate numerical and categorical features\\n\",\n",
    "    \"numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\\n\",\n",
    "    \"categorical_features = X.select_dtypes(include=['object']).columns.tolist()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Numerical features ({len(numerical_features)}): {numerical_features}\\\")\\n\",\n",
    "    \"print(f\\\"Categorical features ({len(categorical_features)}): {categorical_features}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove constant features\\n\",\n",
    "    \"constant_features = []\\n\",\n",
    "    \"for col in X.columns:\\n\",\n",
    "    \"    if X[col].nunique() <= 1:\\n\",\n",
    "    \"        constant_features.append(col)\\n\",\n",
    "    \"\\n\",\n",
    "    \"if constant_features:\\n\",\n",
    "    \"    print(f\\\"\\\\nConstant features to remove: {constant_features}\\\")\\n\",\n",
    "    \"    X = X.drop(columns=constant_features)\\n\",\n",
    "    \"    numerical_features = [col for col in numerical_features if col not in constant_features]\\n\",\n",
    "    \"    categorical_features = [col for col in categorical_features if col not in constant_features]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nFinal feature counts - Numerical: {len(numerical_features)}, Categorical: {len(categorical_features)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Statistical tests for numerical features\\n\",\n",
    "    \"print(\\\"=== STATISTICAL TESTS FOR NUMERICAL FEATURES ===\\\")\\n\",\n",
    "    \"numerical_stats = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for feature in numerical_features:\\n\",\n",
    "    \"    if feature in X.columns:\\n\",\n",
    "    \"        # Remove missing values for analysis\\n\",\n",
    "    \"        feature_data = X[feature].dropna()\\n\",\n",
    "    \"        target_data = y[feature_data.index]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Separate by target classes\\n\",\n",
    "    \"        group_0 = feature_data[target_data == 0]\\n\",\n",
    "    \"        group_1 = feature_data[target_data == 1]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if len(group_0) > 0 and len(group_1) > 0:\\n\",\n",
    "    \"            # T-test\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                t_stat, t_pvalue = ttest_ind(group_0, group_1)\\n\",\n",
    "    \"            except:\\n\",\n",
    "    \"                t_stat, t_pvalue = np.nan, np.nan\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Mann-Whitney U test (non-parametric)\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                u_stat, u_pvalue = mannwhitneyu(group_0, group_1, alternative='two-sided')\\n\",\n",
    "    \"            except:\\n\",\n",
    "    \"                u_stat, u_pvalue = np.nan, np.nan\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            numerical_stats.append({\\n\",\n",
    "    \"                'Feature': feature,\\n\",\n",
    "    \"                'Mean_No_Readmit': group_0.mean(),\\n\",\n",
    "    \"                'Mean_Readmit': group_1.mean(),\\n\",\n",
    "    \"                'Std_No_Readmit': group_0.std(),\\n\",\n",
    "    \"                'Std_Readmit': group_1.std(),\\n\",\n",
    "    \"                'T_Statistic': t_stat,\\n\",\n",
    "    \"                'T_P_Value': t_pvalue,\\n\",\n",
    "    \"                'U_P_Value': u_pvalue,\\n\",\n",
    "    \"                'Effect_Size': abs(group_1.mean() - group_0.mean()) / np.sqrt((group_0.var() + group_1.var()) / 2)\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"\\n\",\n",
    "    \"numerical_stats_df = pd.DataFrame(numerical_stats)\\n\",\n",
    "    \"if len(numerical_stats_df) > 0:\\n\",\n",
    "    \"    numerical_stats_df = numerical_stats_df.sort_values('T_P_Value')\\n\",\n",
    "    \"    print(\\\"Top numerical features by statistical significance:\\\")\\n\",\n",
    "    \"    print(numerical_stats_df.round(4).head(10))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize top significant features\\n\",\n",
    "    \"    significant_features = numerical_stats_df[numerical_stats_df['T_P_Value'] < 0.05].head(6)\\n\",\n",
    "    \"    if len(significant_features) > 0:\\n\",\n",
    "    \"        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\\n\",\n",
    "    \"        axes = axes.ravel()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for i, (_, row) in enumerate(significant_features.iterrows()):\\n\",\n",
    "    \"            feature = row['Feature']\\n\",\n",
    "    \"            feature_data = X[feature].dropna()\\n\",\n",
    "    \"            target_data = y[feature_data.index]\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Box plot\\n\",\n",
    "    \"            data_to_plot = [feature_data[target_data == 0], feature_data[target_data == 1]]\\n\",\n",
    "    \"            axes[i].boxplot(data_to_plot, labels=['No Readmit', 'Readmit'])\\n\",\n",
    "    \"            axes[i].set_title(f'{feature}\\\\n(p-value: {row[\\\"T_P_Value\\\"]:.4f})')\\n\",\n",
    "    \"            axes[i].set_ylabel('Value')\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Statistical tests for categorical features\\n\",\n",
    "    \"print(\\\"\\\\n=== STATISTICAL TESTS FOR CATEGORICAL FEATURES ===\\\")\\n\",\n",
    "    \"categorical_stats = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for feature in categorical_features:\\n\",\n",
    "    \"    if feature in X.columns:\\n\",\n",
    "    \"        # Create contingency table\\n\",\n",
    "    \"        try:\\n\",\n",
    "    \"            # Handle missing values\\n\",\n",
    "    \"            feature_data = X[feature].fillna('Missing')\\n\",\n",
    "    \"            contingency_table = pd.crosstab(feature_data, y)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Chi-square test\\n\",\n",
    "    \"            chi2_stat, chi2_pvalue, dof, expected = chi2_contingency(contingency_table)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            # Cramér's V (effect size)\\n\",\n",
    "    \"            n = contingency_table.sum().sum()\\n\",\n",
    "    \"            cramers_v = np.sqrt(chi2_stat / (n * (min(contingency_table.shape) - 1)))\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            categorical_stats.append({\\n\",\n",
    "    \"                'Feature': feature,\\n\",\n",
    "    \"                'Unique_Values': X[feature].nunique(),\\n\",\n",
    "    \"                'Chi2_Statistic': chi2_stat,\\n\",\n",
    "    \"                'Chi2_P_Value': chi2_pvalue,\\n\",\n",
    "    \"                'Cramers_V': cramers_v,\\n\",\n",
    "    \"                'DOF': dof\\n\",\n",
    "    \"            })\\n\",\n",
    "    \"        except Exception as e:\\n\",\n",
    "    \"            print(f\\\"Error processing {feature}: {e}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"categorical_stats_df = pd.DataFrame(categorical_stats)\\n\",\n",
    "    \"if len(categorical_stats_df) > 0:\\n\",\n",
    "    \"    categorical_stats_df = categorical_stats_df.sort_values('Chi2_P_Value')\\n\",\n",
    "    \"    print(\\\"Top categorical features by statistical significance:\\\")\\n\",\n",
    "    \"    print(categorical_stats_df.round(4).head(10))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize top significant categorical features\\n\",\n",
    "    \"    significant_cat = categorical_stats_df[categorical_stats_df['Chi2_P_Value'] < 0.05].head(4)\\n\",\n",
    "    \"    if len(significant_cat) > 0:\\n\",\n",
    "    \"        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n\",\n",
    "    \"        axes = axes.ravel()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        for i, (_, row) in enumerate(significant_cat.iterrows()):\\n\",\n",
    "    \"            feature = row['Feature']\\n\",\n",
    "    \"            crosstab = pd.crosstab(X[feature].fillna('Missing'), y, normalize='columns')\\n\",\n",
    "    \"            crosstab.plot(kind='bar', ax=axes[i], rot=45)\\n\",\n",
    "    \"            axes[i].set_title(f'{feature}\\\\n(p-value: {row[\\\"Chi2_P_Value\\\"]:.4f})')\\n\",\n",
    "    \"            axes[i].set_ylabel('Proportion')\\n\",\n",
    "    \"            axes[i].legend(['No Readmit', 'Readmit'])\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        plt.tight_layout()\\n\",\n",
    "    \"        plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prepare data for machine learning\\n\",\n",
    "    \"print(\\\"=== PREPARING DATA FOR ML ===\\\")\\n\",\n",
    "    \"X_processed = X.copy()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Fill missing values\\n\",\n",
    "    \"for col in numerical_features:\\n\",\n",
    "    \"    if col in X_processed.columns:\\n\",\n",
    "    \"        X_processed[col] = X_processed[col].fillna(X_processed[col].median())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Encode categorical variables\\n\",\n",
    "    \"label_encoders = {}\\n\",\n",
    "    \"for col in categorical_features:\\n\",\n",
    "    \"    if col in X_processed.columns:\\n\",\n",
    "    \"        le = LabelEncoder()\\n\",\n",
    "    \"        X_processed[col] = X_processed[col].fillna('Unknown')\\n\",\n",
    "    \"        X_processed[col] = le.fit_transform(X_processed[col].astype(str))\\n\",\n",
    "    \"        label_encoders[col] = le\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Processed dataset shape: {X_processed.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Missing values remaining: {X_processed.isnull().sum().sum()}\\\")\\n\",\n",
    "    \"print(f\\\"Features: {list(X_processed.columns)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Random Forest Feature Importance\\n\",\n",
    "    \"print(\\\"=== RANDOM FOREST FEATURE IMPORTANCE ===\\\")\\n\",\n",
    "    \"rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\\n\",\n",
    "    \"rf.fit(X_processed, y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get feature importance\\n\",\n",
    "    \"rf_importance = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_processed.columns,\\n\",\n",
    "    \"    'Importance': rf.feature_importances_\\n\",\n",
    "    \"}).sort_values('Importance', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Top 15 features by Random Forest importance:\\\")\\n\",\n",
    "    \"print(rf_importance.head(15))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize top features\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"top_features = rf_importance.head(15)\\n\",\n",
    "    \"sns.barplot(data=top_features, x='Importance', y='Feature')\\n\",\n",
    "    \"plt.title('Top 15 Features - Random Forest Importance')\\n\",\n",
    "    \"plt.xlabel('Feature Importance')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature importance distribution\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.hist(rf_importance['Importance'], bins=20, alpha=0.7, edgecolor='black')\\n\",\n",
    "    \"plt.title('Distribution of Feature Importance Scores')\\n\",\n",
    "    \"plt.xlabel('Importance Score')\\n\",\n",
    "    \"plt.ylabel('Number of Features')\\n\",\n",
    "    \"plt.axvline(rf_importance['Importance'].mean(), color='red', linestyle='--', label='Mean')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Extra Trees Feature Importance\\n\",\n",
    "    \"print(\\\"\\\\n=== EXTRA TREES FEATURE IMPORTANCE ===\\\")\\n\",\n",
    "    \"et = ExtraTreesClassifier(n_estimators=100, random_state=42, n_jobs=-1)\\n\",\n",
    "    \"et.fit(X_processed, y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"et_importance = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_processed.columns,\\n\",\n",
    "    \"    'Importance': et.feature_importances_\\n\",\n",
    "    \"}).sort_values('Importance', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Top 15 features by Extra Trees importance:\\\")\\n\",\n",
    "    \"print(et_importance.head(15))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compare Random Forest vs Extra Trees importance\\n\",\n",
    "    \"importance_comparison = rf_importance.merge(et_importance, on='Feature', suffixes=('_RF', '_ET'))\\n\",\n",
    "    \"importance_comparison['Avg_Importance'] = (importance_comparison['Importance_RF'] + importance_comparison['Importance_ET']) / 2\\n\",\n",
    "    \"importance_comparison = importance_comparison.sort_values('Avg_Importance', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"top_combined = importance_comparison.head(15)\\n\",\n",
    "    \"x = np.arange(len(top_combined))\\n\",\n",
    "    \"width = 0.35\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.bar(x - width/2, top_combined['Importance_RF'], width, label='Random Forest', alpha=0.8)\\n\",\n",
    "    \"plt.bar(x + width/2, top_combined['Importance_ET'], width, label='Extra Trees', alpha=0.8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.xlabel('Features')\\n\",\n",
    "    \"plt.ylabel('Importance')\\n\",\n",
    "    \"plt.title('Feature Importance Comparison: Random Forest vs Extra Trees')\\n\",\n",
    "    \"plt.xticks(x, top_combined['Feature'], rotation=45, ha='right')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Correlation between RF and ET importance\\n\",\n",
    "    \"plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"plt.scatter(importance_comparison['Importance_RF'], importance_comparison['Importance_ET'], alpha=0.6)\\n\",\n",
    "    \"plt.plot([0, importance_comparison['Importance_RF'].max()], [0, importance_comparison['Importance_RF'].max()], 'r--', alpha=0.8)\\n\",\n",
    "    \"plt.xlabel('Random Forest Importance')\\n\",\n",
    "    \"plt.ylabel('Extra Trees Importance')\\n\",\n",
    "    \"plt.title('Correlation between RF and ET Feature Importance')\\n\",\n",
    "    \"correlation = importance_comparison['Importance_RF'].corr(importance_comparison['Importance_ET'])\\n\",\n",
    "    \"plt.text(0.05, 0.95, f'Correlation: {correlation:.3f}', transform=plt.gca().transAxes, \\n\",\n",
    "    \"         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Feature Selection Techniques\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Univariate Feature Selection\\n\",\n",
    "    \"print(\\\"=== UNIVARIATE FEATURE SELECTION ===\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For numerical features - F-score\\n\",\n",
    "    \"if len(numerical_features) > 0:\\n\",\n",
    "    \"    numerical_data = X_processed[[col for col in numerical_features if col in X_processed.columns]]\\n\",\n",
    "    \"    if len(numerical_data.columns) > 0:\\n\",\n",
    "    \"        f_scores, f_pvalues = f_classif(numerical_data, y)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        f_score_results = pd.DataFrame({\\n\",\n",
    "    \"            'Feature': numerical_data.columns,\\n\",\n",
    "    \"            'F_Score': f_scores,\\n\",\n",
    "    \"            'P_Value': f_pvalues\\n\",\n",
    "    \"        }).sort_values('F_Score', ascending=False)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        print(\\\"\\\\nTop numerical features by F-score:\\\")\\n\",\n",
    "    \"        print(f_score_results.head(10))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For all features - Mutual Information\\n\",\n",
    "    \"mi_scores = mutual_info_classif(X_processed, y, random_state=42)\\n\",\n",
    "    \"mi_results = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_processed.columns,\\n\",\n",
    "    \"    'MI_Score': mi_scores\\n\",\n",
    "    \"}).sort_values('MI_Score', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nTop features by Mutual Information:\\\")\\n\",\n",
    "    \"print(mi_results.head(15))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize MI scores\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"top_mi = mi_results.head(15)\\n\",\n",
    "    \"sns.barplot(data=top_mi, x='MI_Score', y='Feature')\\n\",\n",
    "    \"plt.title('Top 15 Features - Mutual Information Score')\\n\",\n",
    "    \"plt.xlabel('Mutual Information Score')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Recursive Feature Elimination (RFE)\\n\",\n",
    "    \"print(\\\"\\\\n=== RECURSIVE FEATURE ELIMINATION ===\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use Logistic Regression as base estimator\\n\",\n",
    "    \"lr = LogisticRegression(random_state=42, max_iter=1000)\\n\",\n",
    "    \"n_features_to_select = min(15, X_processed.shape[1])\\n\",\n",
    "    \"rfe = RFE(estimator=lr, n_features_to_select=n_features_to_select, step=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Scale features for logistic regression\\n\",\n",
    "    \"scaler = StandardScaler()\\n\",\n",
    "    \"X_scaled = scaler.fit_transform(X_processed)\\n\",\n",
    "    \"\\n\",\n",
    "    \"rfe.fit(X_scaled, y)\\n\",\n",
    "    \"\\n\",\n",
    "    \"rfe_results = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_processed.columns,\\n\",\n",
    "    \"    'Selected': rfe.support_,\\n\",\n",
    "    \"    'Ranking': rfe.ranking_\\n\",\n",
    "    \"}).sort_values('Ranking')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"RFE Selected Features:\\\")\\n\",\n",
    "    \"selected_features = rfe_results[rfe_results['Selected']]['Feature'].tolist()\\n\",\n",
    "    \"for i, feature in enumerate(selected_features, 1):\\n\",\n",
    "    \"    print(f\\\"{i}. {feature}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nRFE Feature Rankings (top 15):\\\")\\n\",\n",
    "    \"print(rfe_results.head(15))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# L1-based Feature Selection (Lasso)\\n\",\n",
    "    \"print(\\\"\\\\n=== L1-BASED FEATURE SELECTION (LASSO) ===\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Use different C values to see feature selection\\n\",\n",
    "    \"C_values = [0.01, 0.1, 1.0, 10.0]\\n\",\n",
    "    \"lasso_results = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for C in C_values:\\n\",\n",
    "    \"    lasso = LogisticRegression(penalty='l1', solver='liblinear', C=C, random_state=42)\\n\",\n",
    "    \"    selector = SelectFromModel(lasso)\\n\",\n",
    "    \"    selector.fit(X_scaled, y)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    selected_features = X_processed.columns[selector.get_support()].tolist()\\n\",\n",
    "    \"    lasso_results[C] = {\\n\",\n",
    "    \"        'n_features': len(selected_features),\\n\",\n",
    "    \"        'features': selected_features\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"C={C}: {len(selected_features)} features selected\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show features selected by most restrictive model\\n\",\n",
    "    \"if lasso_results[0.01]['features']:\\n\",\n",
    "    \"    print(f\\\"\\\\nFeatures selected by Lasso (C=0.01): {lasso_results[0.01]['features']}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"\\\\nFeatures selected by Lasso (C=0.1): {lasso_results[0.1]['features']}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize feature selection across different C values\\n\",\n",
    "    \"plt.figure(figsize=("
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
