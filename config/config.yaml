# Patient Readmission Risk Prediction - Configuration File

# Data Generation Parameters
data_generation:
  n_patients: 10000              # Number of unique patients to generate
  n_admissions: 25000            # Total number of admissions
  readmission_rate: 0.15         # Expected 30-day readmission rate (15%)
  random_seed: 42                # For reproducible results

# Data Cleaning Parameters  
data_cleaning:
  remove_outliers: true          # Remove statistical outliers
  outlier_method: 'iqr'          # Method: 'iqr', 'zscore', or 'isolation'
  outlier_threshold: 3.0         # Threshold for outlier detection
  handle_missing: 'drop'         # 'drop', 'impute', or 'flag'
  min_los: 1                     # Minimum length of stay (days)
  max_los: 30                    # Maximum length of stay (days)

# Feature Engineering Parameters
feature_engineering:
  # Core demographic features
  numerical_features:
    - age
    - length_of_stay
    - previous_admissions
    - avg_previous_los
    - diagnosis_count
    - lab_test_count
    - abnormal_lab_count
    - medication_count
    - procedure_count
    
  categorical_features:
    - gender
    - insurance_type
    - admission_type
    - discharge_disposition
    
  # Feature scaling and encoding
  scale_features: true           # Apply StandardScaler to numerical features
  encode_categorical: 'onehot'   # 'onehot', 'label', or 'target'
  
  # Advanced feature creation
  create_interaction_features: true
  create_polynomial_features: false
  polynomial_degree: 2
  
  # Feature selection
  apply_feature_selection: true
  selection_method: 'mutual_info'  # 'mutual_info', 'chi2', 'f_score'
  n_features_to_select: 20

# Model Training Parameters
model_training:
  # Data splitting
  test_size: 0.2                 # Proportion for test set
  validation_size: 0.2           # Proportion of training set for validation
  random_state: 42               # For reproducible splits
  stratify: true                 # Stratified sampling based on target
  
  # Models to train
  models:
    - logistic
    - random_forest
    - xgboost
    - lightgbm
    
  # Model-specific hyperparameters
  hyperparameters:
    logistic:
      C: [0.1, 1.0, 10.0]
      penalty: ['l1', 'l2']
      solver: ['liblinear']
      max_iter: 1000
      
    random_forest:
      n_estimators: [100, 200, 300]
      max_depth: [10, 20, null]
      min_samples_split: [2, 5, 10]
      min_samples_leaf: [1, 2, 4]
      bootstrap: [true]
      
    xgboost:
      n_estimators: [100, 200, 300]
      max_depth: [3, 6, 10]
      learning_rate: [0.01, 0.1, 0.2]
      subsample: [0.8, 0.9, 1.0]
      colsample_bytree: [0.8, 0.9, 1.0]
      
    lightgbm:
      n_estimators: [100, 200, 300]
      max_depth: [3, 6, 10]
      learning_rate: [0.01, 0.1, 0.2]
      num_leaves: [31, 63, 127]
      subsample: [0.8, 0.9, 1.0]
  
  # Hyperparameter tuning
  hyperparameter_tuning:
    method: 'grid_search'         # 'grid_search', 'random_search', or 'bayesian'
    cv_folds: 5                   # Cross-validation folds
    n_iter: 50                    # For random search
    scoring: 'roc_auc'            # Scoring metric for optimization
    
  # Class imbalance handling
  handle_imbalance: true
  imbalance_method: 'smote'       # 'smote', 'adasyn', 'random_oversample', 'random_undersample'

# Model Evaluation Parameters
model_evaluation:
  # Evaluation metrics
  metrics:
    - accuracy
    - precision
    - recall
    - f1
    - auc
    - log_loss
    
  # Visualization settings
  create_plots: true
  plot_types:
    - confusion_matrix
    - roc_curve
    - precision_recall_curve
    - feature_importance
    - calibration_curve
    
  # Model interpretation
  explain_models: true
  explanation_methods:
    - shap
    - permutation_importance
    
  # Cross-validation for final evaluation
  final_cv_folds: 10
  
# Output and Logging Parameters
output:
  # Directory structure
  base_dir: '.'
  data_dir: 'data'
  models_dir: 'models'
  results_dir: 'results'
  figures_dir: 'figures'
  logs_dir: 'logs'
  
  # File formats
  save_models: true
  model_format: 'pickle'          # 'pickle' or 'joblib'
  save_predictions: true
  save_feature_importance: true
  
  # Logging
  log_level: 'INFO'               # 'DEBUG', 'INFO', 'WARNING', 'ERROR'
  log_to_file: true
  log_to_console: true

# Database Parameters
database:
  type: 'sqlite'
  path: 'data/readmission_db.sqlite'
  echo: false                     # Set to true for SQL query logging
  
# Reproducibility
reproducibility:
  set_seeds: true
  numpy_seed: 42
  random_seed: 42
  tensorflow_seed: 42             # If using TensorFlow in future

# Performance and Resource Management
performance:
  n_jobs: -1                      # Number of parallel jobs (-1 for all cores)
  memory_limit: '8GB'             # Memory limit for large operations
  chunk_size: 10000               # Chunk size for batch processing